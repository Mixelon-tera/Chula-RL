{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo with Gridworld\n",
    "\n",
    "## Goal:\n",
    "\n",
    "- How to implement Monte Carlo algorithm\n",
    "- understandig caveats in RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import chula_rl as rl\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Make Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = rl.env.Gridworld(shape=(4, 3),\n",
    "                           start=(2, 0),\n",
    "                           goal=(1, 2),\n",
    "                           move_reward=-1)\n",
    "    env = rl.env.wrapper.ClipEpisodeLength(env, n_max_length=20)\n",
    "    env = rl.env.wrapper.EpisodeSummary(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define policy\n",
    "\n",
    "First-visit Monte Carlo + Policy iteration + Epsilon greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_return(r, discount_factor):\n",
    "    \"\"\"return G for every time step given a sequence of rewards\"\"\"\n",
    "    # code here ...\n",
    "    # ...\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test calculating return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_return(np.array([1., 1., 1., 1.]), 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:\n",
    "\n",
    "```\n",
    "array([3.439, 2.71 , 1.9  , 1.   ])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_sa(s, a, g):\n",
    "    \"\"\"deduplicate (s, a) keeping only the first occurrances while also matching the corresponding returns\"\"\"\n",
    "    # code here ...\n",
    "    # ...\n",
    "    # return unique sa and g\n",
    "    # sa = tuple(first dim of s, second dim of s, a)\n",
    "    # this is for numpy indexing!\n",
    "    # ex: sa = ([0, 0], [0, 1], [1, 1])\n",
    "    # means: s = [(0, 0), (0, 1)]; a = [1, 1]\n",
    "    return sa, g "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test first_sa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [0, 0],\n",
    "])\n",
    "a = np.array([1, 1, 1])\n",
    "g = np.array([1, 2, 3])\n",
    "sa, g = first_sa(s, a, g)\n",
    "print('sa:', sa)\n",
    "print('g:', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:\n",
    "\n",
    "```\n",
    "sa: (array([0, 0]), array([0, 1]), array([1, 1]))\n",
    "g: [1 2]\n",
    "```\n",
    "\n",
    "Note: The third state and action in sequence is a duplicate. Discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The policy: First-visit Monte Carlo with True Average\n",
    "\n",
    "Note: there are many variants of MC, including:\n",
    "\n",
    "- first or all visits\n",
    "- true average or moving average\n",
    "\n",
    "Here we concern ourselves with first-visit MC with true average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloPolicy(rl.policy.BasePolicy):\n",
    "    \"\"\"firt visit monte carlo with true average\"\"\"\n",
    "\n",
    "    def __init__(self, discount_factor, observation_space, n_action):\n",
    "        self.discount_factor = discount_factor\n",
    "        self.observation_space = observation_space\n",
    "        self.n_action = n_action\n",
    "        # value tables\n",
    "        self.q = np.zeros(list(self.observation_space.high) +\n",
    "                          [n_action])  # (s0, s1, a)\n",
    "        self.cnt = np.zeros(self.q.shape, dtype=int)\n",
    "\n",
    "    def step(self, state):\n",
    "        return np.argmax(self.q[tuple(state)])  # greedy action selection\n",
    "\n",
    "    def optimize_step(self, data):\n",
    "        \"\"\"update the action value (q) table with MC algorithm\"\"\"\n",
    "        s = np.array(data['s'])\n",
    "        a = np.array(data['a'])\n",
    "        r = np.array(data['r'])\n",
    "        \n",
    "        # code here ...\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define an explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = MonteCarloPolicy(discount_factor=0.99,\n",
    "                          observation_space=env.observation_space,\n",
    "                          n_action=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(policy, n_max_interaction):\n",
    "    rl.util.set_seed(0) # predictable results\n",
    "    env = make_env()\n",
    "    explorer = rl.explorer.EpisodeExplorer(n_max_interaction=n_max_interaction, env=env)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            data = explorer.step(policy)\n",
    "            policy.optimize_step(data)  # not defined\n",
    "        except rl.exception.InteractionExceeded:\n",
    "            break\n",
    "    df = pd.DataFrame(explorer.get_stats()['history'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = run(policy, 500)\n",
    "print('max:', stats['reward'].max())\n",
    "stats.plot(x='n_interaction', y='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result: Bad rewards (some -20 reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Why we can't seem to learn a good policy?\n",
    "\n",
    "describe here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Make changes to make the algorithm learn as expected\n",
    "\n",
    "Hint: make the policy more random by adding a random wrapper on top of it ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(rl.policy.BasePolicyWrapper):\n",
    "    \"\"\"wraps around the policy to give the original policy some randomness\"\"\"\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    def step(self, state):\n",
    "        # code here ...\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = MonteCarloPolicy(discount_factor=0.99,\n",
    "                          observation_space=env.observation_space,\n",
    "                          n_action=env.action_space.n)\n",
    "policy = Wrapper(policy)\n",
    "\n",
    "\n",
    "stats = run(policy, 500)\n",
    "print('max:', stats['reward'].max())\n",
    "stats.plot(x='n_interaction', y='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result: You should get max reward close to 2 (much better than previously -21). If not you might want to tune your randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: What is the theoretically maximum reward we could get under this setting with an optimal policy?\n",
    "\n",
    "Describe here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: What will happen in terms of learning if we change the move reward from -1 to 0? Why?\n",
    "\n",
    "That is each move will not be penalized anymore.\n",
    "\n",
    "Describe here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6: After changing move reward to 0 what is now the theoretically maximum reward attainable?\n",
    "\n",
    "Describe here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: What would happen in terms of learning if we don't clip the episode?\n",
    "\n",
    "Describe here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
