{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA with Gridworld\n",
    "\n",
    "## Goal:\n",
    "\n",
    "- Adapt from Monte Carlo\n",
    "- SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import chula_rl as rl\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Make Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = rl.env.Gridworld(shape=(4, 3),\n",
    "                           start=(2, 0),\n",
    "                           goal=(1, 2),\n",
    "                           move_reward=-1)\n",
    "    env = rl.env.wrapper.ClipEpisodeLength(env, n_max_length=20)\n",
    "    env = rl.env.wrapper.EpisodeSummary(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define policy\n",
    "\n",
    "SARSA with policy iteration (with only one-step policy evaluation) using moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAPolicy(rl.policy.BasePolicy):\n",
    "\n",
    "    def __init__(self, lr, discount_factor, observation_space, n_action):\n",
    "        self.lr = lr\n",
    "        self.discount_factor = discount_factor\n",
    "        self.observation_space = observation_space\n",
    "        self.n_action = n_action\n",
    "\n",
    "        self.q = np.zeros(list(self.observation_space.high) +\n",
    "                          [n_action])  # (s0, s1, a)\n",
    "\n",
    "    def step(self, state):\n",
    "        # code here ...\n",
    "        # ...\n",
    "        pass\n",
    "\n",
    "    def optimize_step(self, data):\n",
    "        # code here ...\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define an explorer\n",
    "\n",
    "Create `OneStepExplorer` to use with SARSA. You could look at the `EpisodeExplorer` for inspirations.\n",
    "\n",
    "The `OneStepExplorer` should run a given policy only for ONE step then return what it sees.\n",
    "\n",
    "It should return something like this from its `step` method: \n",
    "\n",
    "```\n",
    "{\n",
    "    's': current state,\n",
    "    'a': action,\n",
    "    'r': reward,\n",
    "    'ss': next state,\n",
    "    'aa': next action,\n",
    "    'done': is done?,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepExplorer(rl.explorer.BaseExplorer):\n",
    "    \"\"\"explore the environment only one-step then returns the experience immediately\"\"\"\n",
    "    def __init__(self, n_max_interaction, env):\n",
    "        super().__init__(env)\n",
    "        self.n_max_interaction = n_max_interaction\n",
    "        self.last_s = self.env.reset()\n",
    "        self.n_interaction = 0\n",
    "        self.n_ep = 0\n",
    "\n",
    "    def step(self, policy):\n",
    "        if self.n_interaction >= self.n_max_interaction:\n",
    "            raise rl.exception.InteractionExceeded()\n",
    "            \n",
    "        # code here\n",
    "        # ...\n",
    "        # data = the experience of the last step\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the explorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.util.set_seed(0)\n",
    "explorer = OneStepExplorer(1, make_env())\n",
    "explorer.step(rl.policy.RandomPolicy(n_action=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result: \n",
    "\n",
    "```\n",
    "{'s': array([2, 0]),\n",
    " 'a': 3,\n",
    " 'r': -1,\n",
    " 'ss': array([2, 1]),\n",
    " 'aa': 3,\n",
    " 'done': False}\n",
    "```\n",
    "\n",
    "Your result might vary, but the structure should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer.step(rl.policy.RandomPolicy(n_action=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result: `InteractionExceeded` exception raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(policy, n_max_interaction):\n",
    "    rl.util.set_seed(0) # predictable results\n",
    "    env = make_env()\n",
    "    explorer = OneStepExplorer(n_max_interaction=n_max_interaction, env=env)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            data = explorer.step(policy)\n",
    "            policy.optimize_step(data)  # not defined\n",
    "        except rl.exception.InteractionExceeded:\n",
    "            break\n",
    "    df = pd.DataFrame(explorer.get_stats()['history'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = SARSAPolicy(lr=0.1,\n",
    "                     discount_factor=0.99,\n",
    "                     observation_space=env.observation_space,\n",
    "                     n_action=env.action_space.n)\n",
    "\n",
    "stats = run(policy, 500)\n",
    "print('max:', stats['reward'].max())\n",
    "stats.plot(x='n_interaction', y='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result: In our implementation, we could get ~2 reward under 500 iteractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Even without an explicit exploration like epsilon greedy, SARSA seems to work! Why is that the case? What works as an \"implicit\" exploration?\n",
    "\n",
    "Describe here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Show us then when an explicit exploration would be important? \n",
    "\n",
    "Create code blocks (and run them) as needed to proof your argument by comparing results SARSA vs. SARSA + epsilon greedy \n",
    "\n",
    "Plots are welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy(rl.policy.BasePolicyWrapper):\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    def step(self, state):\n",
    "        # code here ...\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create blocks as needed ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
