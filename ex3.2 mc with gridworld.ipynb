{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo with Gridworld\n",
    "\n",
    "## Goal:\n",
    "\n",
    "- How to implement Monte Carlo algorithm\n",
    "- understandig caveats in RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import chula_rl as rl\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Make Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_env():\n",
    "    env = rl.env.Gridworld(shape=(4, 3),\n",
    "                           start=(2, 0),\n",
    "                           goal=(1, 2),\n",
    "                           move_reward=-1)\n",
    "    env = rl.env.wrapper.ClipEpisodeLength(env, n_max_length=20)\n",
    "    env = rl.env.wrapper.EpisodeSummary(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define policy\n",
    "\n",
    "First-visit Monte Carlo + Policy iteration + Epsilon greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_return(r, discount_factor):\n",
    "    \"\"\"return G for every time step given a sequence of rewards\"\"\"\n",
    "    # code here ...\n",
    "    # ...\n",
    "    g = []\n",
    "    temp = 0\n",
    "    for n in range(len(r)):\n",
    "        temp += pow(discount_factor,n) * r[n]\n",
    "        g.append(temp)\n",
    "    g = np.flip(np.array(g))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test calculating return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.439, 2.71 , 1.9  , 1.   ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_return(np.array([1., 1., 1., 1.]), 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:\n",
    "\n",
    "```\n",
    "array([3.439, 2.71 , 1.9  , 1.   ])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_sa(s, a, g):\n",
    "    \"\"\"deduplicate (s, a) keeping only the first occurrances while also matching the corresponding returns\"\"\"\n",
    "    # code here ...\n",
    "    # ...\n",
    "    # return unique sa and g\n",
    "    # sa = tuple(first dim of s, second dim of s, a)\n",
    "    # this is for numpy indexing!\n",
    "    # ex: sa = ([0, 0], [0, 1], [1, 1])\n",
    "    # means: s = [(0, 0), (0, 1)]; a = [1, 1]\n",
    "    \n",
    "    count = 0\n",
    "    running = True\n",
    "    sa = []\n",
    "    \n",
    "    for state in s:\n",
    "        if not running:\n",
    "            break\n",
    "        if len(sa)>0 and running:\n",
    "            for i in sa:\n",
    "                if not (state[0]==i[0] and state[1]==i[1]):\n",
    "                    sa.append(state)\n",
    "                    count += 1\n",
    "                else:\n",
    "                    sa.append(a[:count])\n",
    "                    running=False\n",
    "                    break\n",
    "        else:\n",
    "            sa.append(state)\n",
    "            count += 1\n",
    "                \n",
    "            \n",
    "    sa = tuple(sa)\n",
    "    g = g[:count]\n",
    "    \n",
    "    return tuple(sa), g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa: (array([0, 0]), array([0, 1]), array([1, 1]))\n",
      "g: [1 2]\n"
     ]
    }
   ],
   "source": [
    "# test on function\n",
    "s = np.array([(0, 0), (0, 1)])\n",
    "a = np.array([1, 1])\n",
    "g = np.array([1,2])\n",
    "sa , g = first_sa(s, a, g)\n",
    "print('sa:', sa)\n",
    "print('g:', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test first_sa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa: (array([0, 0]), array([0, 1]), array([1, 1]))\n",
      "g: [1 2]\n"
     ]
    }
   ],
   "source": [
    "s = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [0, 0],\n",
    "])\n",
    "a = np.array([1, 1, 1])\n",
    "g = np.array([1, 2, 3])\n",
    "sa, g = first_sa(s, a, g)\n",
    "print('sa:', sa)\n",
    "print('g:', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:\n",
    "\n",
    "```\n",
    "sa: (array([0, 0]), array([0, 1]), array([1, 1]))\n",
    "g: [1 2]\n",
    "```\n",
    "\n",
    "Note: The third state and action in sequence is a duplicate. Discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The policy: First-visit Monte Carlo with True Average\n",
    "\n",
    "Note: there are many variants of MC, including:\n",
    "\n",
    "- first or all visits\n",
    "- true average or moving average\n",
    "\n",
    "Here we concern ourselves with first-visit MC with true average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloPolicy(rl.policy.BasePolicy):\n",
    "    \"\"\"firt visit monte carlo with true average\"\"\"\n",
    "\n",
    "    def __init__(self, discount_factor, observation_space, n_action):\n",
    "        self.discount_factor = discount_factor\n",
    "        self.observation_space = observation_space\n",
    "        self.n_action = n_action\n",
    "        # value tables\n",
    "        self.q = np.zeros(list(self.observation_space.high) +\n",
    "                          [n_action])  # (s0, s1, a)\n",
    "        self.cnt = np.zeros(self.q.shape, dtype=int)\n",
    "\n",
    "    def step(self, state):\n",
    "        return np.argmax(self.q[tuple(state)])  # greedy action selection\n",
    "\n",
    "    def optimize_step(self, data):\n",
    "        \"\"\"update the action value (q) table with MC algorithm\"\"\"\n",
    "        s = np.array(data['s'])\n",
    "        a = np.array(data['a'])\n",
    "        r = np.array(data['r'])\n",
    "        \n",
    "        # code here ...\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define an explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = MonteCarloPolicy(discount_factor=0.99,\n",
    "                          observation_space=env.observation_space,\n",
    "                          n_action=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(policy, n_max_interaction):\n",
    "    rl.util.set_seed(0) # predictable results\n",
    "    env = make_env()\n",
    "    explorer = rl.explorer.EpisodeExplorer(n_max_interaction=n_max_interaction, env=env)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            data = explorer.step(policy)\n",
    "            policy.optimize_step(data)  # not defined\n",
    "        except rl.exception.InteractionExceeded:\n",
    "            break\n",
    "    df = pd.DataFrame(explorer.get_stats()['history'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = run(policy, 500)\n",
    "print('max:', stats['reward'].max())\n",
    "stats.plot(x='n_interaction', y='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result: Bad rewards (some -20 reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Why we can't seem to learn a good policy?\n",
    "\n",
    "describe here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Make changes to make the algorithm learn as expected\n",
    "\n",
    "Hint: make the policy more random by adding a random wrapper on top of it ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(rl.policy.BasePolicyWrapper):\n",
    "    \"\"\"wraps around the policy to give the original policy some randomness\"\"\"\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    def step(self, state):\n",
    "        # code here ...\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = MonteCarloPolicy(discount_factor=0.99,\n",
    "                          observation_space=env.observation_space,\n",
    "                          n_action=env.action_space.n)\n",
    "policy = Wrapper(policy)\n",
    "\n",
    "\n",
    "stats = run(policy, 500)\n",
    "print('max:', stats['reward'].max())\n",
    "stats.plot(x='n_interaction', y='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result: You should get max reward close to 2 (much better than previously -21). If not you might want to tune your randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: What is the theoretically maximum reward we could get under this setting with an optimal policy?\n",
    "\n",
    "Describe here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: What will happen in terms of learning if we change the move reward from -1 to 0? Why?\n",
    "\n",
    "That is each move will not be penalized anymore.\n",
    "\n",
    "Describe here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6: After changing move reward to 0 what is now the theoretically maximum reward attainable?\n",
    "\n",
    "Describe here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: What would happen in terms of learning if we don't clip the episode?\n",
    "\n",
    "Describe here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
